{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music Multiple - Style-Controlled Music Generation\n",
    "Welcome to **Music Multiple**'s MusicGen-Style demo notebook. Here you will find self-contained examples of how to use style-controlled music generation within the Music Multiple ecosystem.\n",
    "\n",
    "**Music Multiple** introduces advanced style conditioning for precise musical control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Music Multiple - Style Model Initialization\n",
    "from audiocraft.models import MusicGen\n",
    "from audiocraft.models import MultiBandDiffusion\n",
    "\n",
    "USE_DIFFUSION_DECODER = False\n",
    "\n",
    "model = MusicGen.get_pretrained('facebook/musicgen-style')\n",
    "if USE_DIFFUSION_DECODER:\n",
    "    mbd = MultiBandDiffusion.get_mbd_musicgen()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "Next, let us configure the generation parameters. You can control:\n",
    "* `use_sampling` (bool): Use sampling if True, else argmax decoding\n",
    "* `top_k` (int): top_k for sampling\n",
    "* `top_p` (float): top_p for sampling\n",
    "* `temperature` (float): Softmax temperature parameter\n",
    "* `duration` (float): Duration of generated waveform\n",
    "* `cfg_coef` (float): Classifier free guidance coefficient\n",
    "* `cfg_coef_beta` (float): Double CFG parameter for text conditioning boost\n",
    "\n",
    "### Style Conditioner Parameters\n",
    "* `eval_q` (int): Quantization level (1-6) - higher values pass more style information\n",
    "* `excerpt_length` (float): Audio excerpt length for style extraction (1.5-4.5 seconds)\n",
    "\n",
    "**Music Multiple Tip:** Use `cfg_coef_beta` to balance text vs style conditioning in combined generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Music Multiple - Basic Generation Configuration\n",
    "model.set_generation_params(\n",
    "    use_sampling=True,\n",
    "    top_k=250,\n",
    "    duration=30\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation Modes\n",
    "**Music Multiple** supports three advanced generation modes:\n",
    "* **Text-to-Music**: Standard text conditioning\n",
    "* **Style-to-Music**: Generate music matching a reference audio style\n",
    "* **Text-and-Style-to-Music**: Combine text descriptions with audio style references\n",
    "\n",
    "All modes use `model.generate_with_chroma`, with optional parameters for style conditioning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text-to-Music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Music Multiple - Text-Only Generation\n",
    "from audiocraft.utils.notebook import display_audio\n",
    "\n",
    "model.set_generation_params(\n",
    "    duration=8, # generate 8 seconds, can go up to 30\n",
    "    use_sampling=True, \n",
    "    top_k=250,\n",
    "    cfg_coef=3., # Classifier Free Guidance coefficient \n",
    "    cfg_coef_beta=None, # double CFG is only useful for text-and-style conditioning\n",
    ")\n",
    "\n",
    "output = model.generate(\n",
    "    descriptions=[\n",
    "        '80s pop track with bassy drums and synth',\n",
    "        '90s rock song with loud guitars and heavy drums',\n",
    "        'Progressive rock drum and bass solo',\n",
    "        'Punk Rock song with loud drum and power guitar',\n",
    "        'Bluesy guitar instrumental with soulful licks and a driving rhythm section',\n",
    "        'Jazz Funk song with slap bass and powerful saxophone',\n",
    "        'drum and bass beat with intense percussions'\n",
    "    ],\n",
    "    progress=True, return_tokens=True\n",
    ")\n",
    "display_audio(output[0], sample_rate=32000)\n",
    "if USE_DIFFUSION_DECODER:\n",
    "    out_diffusion = mbd.tokens_to_wav(output[1])\n",
    "    display_audio(out_diffusion, sample_rate=32000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Style-to-Music\n",
    "Generate music that matches the style of a reference audio without text descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Music Multiple - Style-Only Generation\n",
    "import torchaudio\n",
    "from audiocraft.utils.notebook import display_audio\n",
    "\n",
    "model.set_generation_params(\n",
    "    duration=8, # generate 8 seconds, can go up to 30\n",
    "    use_sampling=True, \n",
    "    top_k=250,\n",
    "    cfg_coef=3., # Classifier Free Guidance coefficient \n",
    "    cfg_coef_beta=None, # double CFG is only useful for text-and-style conditioning\n",
    ")\n",
    "\n",
    "model.set_style_conditioner_params(\n",
    "    eval_q=1, # integer between 1 and 6\n",
    "              # eval_q is the level of quantization that passes\n",
    "              # through the conditioner. When low, the models adheres less to the \n",
    "              # audio conditioning\n",
    "    excerpt_length=3., # the length in seconds that is taken by the model in the provided excerpt\n",
    "    )\n",
    "\n",
    "melody_waveform, sr = torchaudio.load(\"../assets/electronic.mp3\")\n",
    "melody_waveform = melody_waveform.unsqueeze(0).repeat(2, 1, 1)\n",
    "output = model.generate_with_chroma(\n",
    "    descriptions=[None, None], \n",
    "    melody_wavs=melody_waveform,\n",
    "    melody_sample_rate=sr,\n",
    "    progress=True, return_tokens=True\n",
    ")\n",
    "display_audio(output[0], sample_rate=32000)\n",
    "if USE_DIFFUSION_DECODER:\n",
    "    out_diffusion = mbd.tokens_to_wav(output[1])\n",
    "    display_audio(out_diffusion, sample_rate=32000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text-and-Style-to-Music\n",
    "**Music Multiple Advanced Feature:** Combine text descriptions with audio style references using Double Classifier Free Guidance.\n",
    "\n",
    "The double CFG formula:\n",
    "$$l_{\\text{double CFG}} = l_{\\emptyset} + \\alpha [l_{style} + \\beta(l_{text, style} - l_{style}) - l_{\\emptyset}]$$\n",
    "\n",
    "Where $\\beta > 1$ boosts text conditioning influence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Music Multiple - Combined Text and Style Generation\n",
    "import torchaudio\n",
    "from audiocraft.utils.notebook import display_audio\n",
    "\n",
    "model.set_generation_params(\n",
    "    duration=8, # generate 8 seconds, can go up to 30\n",
    "    use_sampling=True, \n",
    "    top_k=250,\n",
    "    cfg_coef=3., # Classifier Free Guidance coefficient \n",
    "    cfg_coef_beta=5., # double CFG is necessary for text-and-style conditioning\n",
    "                   # Beta in the double CFG formula. between 1 and 9. When set to 1 \n",
    "                   # it is equivalent to normal CFG. \n",
    ")\n",
    "\n",
    "model.set_style_conditioner_params(\n",
    "    eval_q=1, # integer between 1 and 6\n",
    "              # eval_q is the level of quantization that passes\n",
    "              # through the conditioner. When low, the models adheres less to the \n",
    "              # audio conditioning\n",
    "    excerpt_length=3., # the length in seconds that is taken by the model in the provided excerpt\n",
    "    )\n",
    "\n",
    "melody_waveform, sr = torchaudio.load(\"../assets/electronic.mp3\")\n",
    "melody_waveform = melody_waveform.unsqueeze(0).repeat(3, 1, 1)\n",
    "\n",
    "descriptions = [\"8-bit old video game music\", \"Chill lofi remix\", \"80s New wave with synthesizer\"]\n",
    "\n",
    "output = model.generate_with_chroma(\n",
    "    descriptions=descriptions,\n",
    "    melody_wavs=melody_waveform,\n",
    "    melody_sample_rate=sr,\n",
    "    progress=True, return_tokens=True\n",
    ")\n",
    "display_audio(output[0], sample_rate=32000)\n",
    "if USE_DIFFUSION_DECODER:\n",
    "    out_diffusion = mbd.tokens_to_wav(output[1])\n",
    "    display_audio(out_diffusion, sample_rate=32000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéµ About Music Multiple - Style Control\n",
    "\n",
    "**Music Multiple** provides advanced style conditioning for precise musical control:\n",
    "\n",
    "### üé≠ Generation Modes\n",
    "- **Text-to-Music**: Traditional text-based generation\n",
    "- **Style-to-Music**: Extract and replicate audio style characteristics\n",
    "- **Hybrid Generation**: Combine text and style for precise control\n",
    "\n",
    "### ‚öôÔ∏è Style Conditioner Parameters\n",
    "- **eval_q (1-6)**: Controls style adherence level\n",
    "  - Lower values: More creative freedom\n",
    "  - Higher values: Closer style matching\n",
    "- **excerpt_length (1.5-4.5s)**: Audio segment used for style extraction\n",
    "\n",
    "### üéõÔ∏è Double CFG Technology\n",
    "- **Balanced Control**: Fine-tune text vs style influence\n",
    "- **Mathematical Precision**: Advanced conditioning formula\n",
    "- **Creative Flexibility**: Adjustable text emphasis\n",
    "\n",
    "### üí° Usage Guidelines\n",
    "- Start with `eval_q=1` and increase for stronger style matching\n",
    "- Use `cfg_coef_beta=5` as starting point for hybrid generation\n",
    "- Adjust `cfg_coef_beta` based on results:\n",
    "  - Too much text adherence ‚Üí Decrease beta\n",
    "  - Too much style adherence ‚Üí Increase beta\n",
    "- Experiment with different excerpt lengths for varied style capture\n",
    "\n",
    "### üéØ Professional Applications\n",
    "- **Music Production**: Maintain consistent style across tracks\n",
    "- **Content Creation**: Match specific audio aesthetics\n",
    "- **Sound Design**: Precise control over musical characteristics\n",
    "- **Creative Exploration**: Combine disparate styles and descriptions\n",
    "\n",
    "*Part of the Music Multiple ecosystem - Advanced style-controlled music generation*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "music_multiple_style",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "b02c911f9b3627d505ea4a19966a915ef21f28afb50dbf6b2115072d27c69103"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}